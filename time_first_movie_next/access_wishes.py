import os
import json
import time
import warnings
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from utils.utils import new_session, running_min, dict_ott
warnings.filterwarnings('ignore')


def get_path(relative_path):
    BASE_DIR = os.path.abspath(os.path.dirname(__file__)) 
    return os.path.normpath(os.path.join(BASE_DIR, relative_path)) 


def load_user_key():
    user_email = input('ì™“ì± í”¼ë””ì•„ ID : ')
    with open(get_path("data/user/user_info.json"), "r") as f:
        user_info = json.load(f)
    user_key = user_info[user_email]

    return user_key


def scrape_wishes(driver, user_key):
    """ìœ ì €ì˜ 'ë³´ê³ ì‹¶ì–´ìš”' ëª©ë¡ ì ‘ê·¼"""
    
    # driver = webdriver.Chrome()
    
    url = f'https://pedia.watcha.com/ko-KR/users/{user_key}/contents/movies/wishes?order=runtime_short'
    driver.get(url)
    time.sleep(2)

    # í˜ì´ì§€ ë¡œë“œ
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)
        new_height = driver.execute_script("return document.body.scrollHeight")
        time.sleep(1)
        if new_height == last_height:
            break
        last_height = new_height

    soup = BeautifulSoup(driver.page_source)
    raw = soup.find_all("li", class_="zK9dEEA5")
    contents = [r.find("a").get("href").split('/')[-1] for r in raw]

    # driver.quit()

    return contents


def update_data(user_key, df, contents):
    """ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµ í›„ ì—…ë°ì´íŠ¸"""

    existing_contents = set(df['content_id'].tolist())
    contents_to_add = list(set(contents) - existing_contents)
    contents_to_remove = list(existing_contents - set(contents))

    df = df.loc[~df['content_id'].isin(contents_to_remove)].reset_index(drop=True)

    session = new_session()

    for content_id in contents_to_add:
        url = f'https://pedia.watcha.com/ko-KR/contents/{content_id}'
        response = session.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        try:
            raw = soup.find("div", class_="sEtpm1Zv J0ALB7d7")
            raw1 = raw.find("div")

            title_ko = raw1.find("h1").get_text()
            try:
                [[title_en], [year, _, country], [running_time, age]] = [r.get_text().split(' Â· ') for r in raw1.find_all("div")[:3]]
            except:
                [[title_en], [year, _, country], [running_time]] = [r.get_text().split(' Â· ') for r in raw1.find_all("div")[:3]]

            running_mins = running_min(running_time)

            if raw.find("ul") is None:
                ott_img, ott_tag = [], ''
            else:
                ott_img = [r.find("div").get("style").split('("')[1].split('")')[0] for r in raw.find("ul").find_all("a")]
                ott_list = [dict_ott.get(img, "Unknown") for img in ott_img]
                ott_tag = '#' + '#'.join(ott_list)

            avg_score = soup.find("div", class_="tG5M99eW PZqhmqar").get_text() if soup.find("div", class_="tG5M99eW PZqhmqar") else ''
            img = soup.find("img").get("src")

            df.loc[len(df)] = [content_id, title_ko, title_en, running_mins, running_time, year, country, age, ott_img, ott_tag, avg_score, img]

        except:
            continue

    return df


def runningtime_categories(df, user_key):
    """ëŸ¬ë‹íƒ€ì„ì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬"""

    df_final = df.copy()

    bins1 = [0] + list(range(60, 240, 30)) + [900]; labels1 = [0] + list(range(60, 240, 30))  # ì´ìƒ
    df_final['time_type1'] = pd.cut(df_final['running_min'], bins=bins1, labels=labels1).astype(float)

    bins2 = [0, 30] + list(range(60, 240, 10)) + [900]; labels2 = [30] + list(range(60, 250, 10))  # ì´í•˜
    df_final['time_type2'] = pd.cut(df_final['running_min'], bins=bins2, labels=labels2).astype(float)

    dict_cat = dict(zip(labels1, ['1ì‹œê°„ ì´í•˜', '1ì‹œê°„ ~ 1ì‹œê°„ 30ë¶„', '1ì‹œê°„ 30ë¶„ ~ 2ì‹œê°„', 
                                  '2ì‹œê°„ ~ 2ì‹œê°„ 30ë¶„', '2ì‹œê°„ 30ë¶„ ~ 3ì‹œê°„', '3ì‹œê°„ ~ 3ì‹œê°„ 30ë¶„', '3ì‹œê°„ 30ë¶„ ì´ìƒ']))
    df_final['category_1d'] = df_final['time_type1'].map(dict_cat)
    df_final['category_2d'] = df_final['time_type2'] - df_final['time_type1']

    return df_final


def main():
    try:
        # ìœ ì € í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
        user_key = load_user_key()

        # ì°œí•œ ì˜í™” ëª©ë¡ ìŠ¤í¬ë˜í•‘
        contents = scrape_wishes(user_key)

        # ë°ì´í„° ì—…ë°ì´íŠ¸
        df = update_data(user_key, contents)

        # ëŸ¬ë‹íƒ€ì„ ì¹´í…Œê³ ë¦¬ ìƒì„± í›„ ì €ì¥
        runningtime_categories(df, user_key)

    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
